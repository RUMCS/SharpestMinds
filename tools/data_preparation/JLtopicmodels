## R program to extract 5-10 topics from corpus.JSS_papers
## J Lopez - extracted from paper "topicmodels: An R Package for Fitting Topic Models"h
##
### 
JSS_papers <- JSS_papers[JSS_papers[,"date"] < "2010-08-05",] 
JSS_papers <- JSS_papers[sapply(JSS_papers[, "description"], Encoding) == "unknown",]

library("tm") 
library("XML") 

remove_HTML_markup <- function(s) tryCatch({ doc <- htmlTreeParse(paste("<!DOCTYPE html>", sep= " "),
+                                             asText = TRUE, trim = FALSE) 
+                                             xmlValue(xmlRoot(doc)) 
+                                          }, error = function(s) s)

corpus <- Corpus(VectorSource(sapply(JSS_papers[, "description"], remove_HTML_markup)))

## The corpus is exported to a document-term matrix using function DocumentTermMatrix() 
## from package tm. The terms are stemmed and the stop words, punctuation, numbers and 
## terms of length less than 3 are removed using the control argument. 
## (We use a C locale for reproducibility.)

Sys.setlocale("LC_COLLATE", "C") ## Get details of or set aspects of the locale for the R process.

JSS_dtm <- DocumentTermMatrix(corpus, control = list(stemming = TRUE, stopwords = TRUE, minWordLength = 3, 
+                             removeNumbers = TRUE, removePunctuation = TRUE)) 

dim(JSS_dtm)

## The mean term frequency-inverse document frequency (tf-idf) over documents containing this term 
## is used to select the vocabulary. This measure allows to omit terms which have low frequency as 
## well as those occurring in many documents. We only include terms which have a tf-idf value of at 
## least 0.1 which is a bit more than the median and ensures that the very frequent terms are omitted.

library("slam") 

summary(col_sums(JSS_dtm))

term_tfidf <- tapply(JSS_dtm$v/row_sums(JSS_dtm)[JSS_dtm$i], JSS_dtm$j, mean) * 
+             log2(nDocs(JSS_dtm)/col_sums(JSS_dtm > 0))

summary(term_tfidf)

JSS_dtm <- JSS_dtm[,term_tfidf >= 0.1] 
JSS_dtm <- JSS_dtm[row_sums(JSS_dtm) > 0,] 
summary(col_sums(JSS_dtm))

## After this pre-processing we have the following document-term matrix with a reduced vocabulary 
## which we can use to ﬁt topic models.

dim(JSS_dtm)

library("topicmodels") 
k <- 30 
SEED <- 2010 
jss_TM <- list(VEM = LDA(JSS_dtm, k = k, control = list(seed = SEED)), 
+ VEM_fixed = LDA(JSS_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)), 
+ Gibbs = LDA(JSS_dtm, k = k, method = "Gibbs", 
+ control = list(seed = SEED, burnin = 1000, 
+ thin = 100, iter = 1000)), 
+ CTM = CTM(JSS_dtm, k = k, 
+ control = list(seed = SEED, 
+ var = list(tol = 10^-4), em = list(tol = 10^-3))))

## To compare the ﬁtted models we ﬁrst investigate the α values 
## of the models ﬁtted with VEM and α estimated and with VEM and α ﬁxed.

## sapply is a user-friendly version and wrapper of lapply by default returning a vector, matrix or, 
## if simplify = "array", an array if appropriate, by applying simplify2array(). 
## sapply(x, f, simplify = FALSE, USE.NAMES = FALSE) is the same as lapply(x, f).

sapply(jss_TM[1:2], slot, "alpha")

sapply(jss_TM, function(x) 
+ mean(apply(posterior(x)$topics, 
+ 1, function(z) - sum(z * log(z)))))

## The estimated topics for a document and estimated terms for a topic can be obtained using the convenience 
## functions topics() and terms(). The most likely topic for each document is obtained by

Topic <- topics(jss_TM[["VEM"]], 1)

## The ﬁve most frequent terms for each topic are obtained by

Terms <- terms(jss_TM[["VEM"]], 5) 
Terms[,1:5]
